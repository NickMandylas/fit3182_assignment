{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT3182 - Assignment 2\n",
    "---\n",
    "### Part B - Streaming Application\n",
    "\n",
    "**Information:**\n",
    "- Filename: Assignment_PartB_Streaming_Application.ipynb\n",
    "- Student Name: Nicholas Mandylas\n",
    "- Student Number: 27840328\n",
    "- Student Email: nman48@student.monash.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import pygeohash as pgh\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pymongo import MongoClient\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "def geohash_handler(latitude, longitude):\n",
    "    return pgh.encode(latitude, longitude, precision=3)\n",
    "\n",
    "\n",
    "def hotspots_handler(hotspots_aqua, hotspots_terra):\n",
    "    # If there are no hotspots from either satellite, return empty array.\n",
    "    if len(hotspots_aqua) == 0 and len(hotspots_terra) == 0:\n",
    "        return []\n",
    "    # If there are only hotspots from a single satellite, then there is opportunity for a location\n",
    "    elif len(hotspots_aqua) > 0 and len(hotspots_terra) == 0:\n",
    "        # to be close to another satellite, so return only the statellite that containes data.\n",
    "        return hotspots_aqua\n",
    "    elif len(hotspots_aqua) == 0 and len(hotspots_terra) > 0:\n",
    "        return hotspots_terra\n",
    "    else:\n",
    "        hotspots = []\n",
    "\n",
    "        for aqua in hotspots_aqua:\n",
    "            count = 0\n",
    "\n",
    "            # Loop through both arrays, to see if there's a location that is close by.\n",
    "            # If an aqua satellite location is close to a terra satellite location, take the average for confidence and surface_temp,\n",
    "            # I then append the new hotspot data to the hotspots final array, then pop the terra satellite data from the hotspots_terra array,\n",
    "            # So there's no duplicate matches in the future. (Task was only to match TWO satellite locations).\n",
    "            while count < len(hotspots_terra):\n",
    "                terra = hotspots_terra[count]\n",
    "                if aqua['geo_hash'] == terra['geo_hash']:\n",
    "                    avg_hotspot = aqua\n",
    "                    avg_hotspot['confidence'] = (\n",
    "                        aqua['confidence'] + terra['confidence']) / 2\n",
    "                    avg_hotspot['surface_temperature_celcius'] = (\n",
    "                        aqua['surface_temperature_celcius'] + terra['surface_temperature_celcius']) / 2\n",
    "                    hotspots_terra.pop(count)\n",
    "                    hotspots.append(avg_hotspot)\n",
    "                    break\n",
    "                else:\n",
    "                    # If no close satellites, append aqua to final array.\n",
    "                    hotspots.append(aqua)\n",
    "                count += 1\n",
    "\n",
    "        # If there are terra satellites that haven't popped, we append them to the final array.\n",
    "        if len(hotspots_terra) > 0:\n",
    "            for terra in hotspots_terra:\n",
    "                hotspots.append(terra)\n",
    "\n",
    "        return hotspots\n",
    "\n",
    "\n",
    "def climate_handler(climate, hotspots):\n",
    "    # If there was no climate, return empty dictionary.\n",
    "    if len(hotspots) > 0 and climate != {}:\n",
    "        for hotspot in hotspots:\n",
    "            # Check if climate & hotspot are close.\n",
    "            if climate['geo_hash'] == hotspot['geo_hash']:\n",
    "                # Check if natural or other\n",
    "                if climate['air_temperature_celcius'] > 20 and climate['ghi'] > 180:\n",
    "                    hotspot['cause'] = 'natural'\n",
    "                else:\n",
    "                    hotspot['cause'] = 'other'\n",
    "\n",
    "                if 'hotspots' in climate:  # Append hotspot\n",
    "                    climate['hotspots'].append(hotspot)\n",
    "                else:\n",
    "                    climate['hotspots'] = [hotspot]\n",
    "\n",
    "    climate['station'] = 948700  # Station number required for DB data model.\n",
    "\n",
    "    return climate\n",
    "\n",
    "\n",
    "def stream_handler(iter):\n",
    "\n",
    "    hotspots_aqua = []\n",
    "    hotspots_terra = []\n",
    "    climate = {}\n",
    "\n",
    "    for each in iter:  # For each item from the data batch.\n",
    "        # Deserialise the data from the Kafka stream.\n",
    "        data = json.loads(each[1])\n",
    "        # Calculate and set the geo-hash.\n",
    "        data['geo_hash'] = geohash_handler(data['latitude'], data['longitude'])\n",
    "        producer_id = data['producer_id']\n",
    "\n",
    "        # Sort data depending on the producer_id (i.e. where it came from.)\n",
    "        if producer_id == 'producer_climate':\n",
    "            climate = data\n",
    "        elif producer_id == 'producer_hotspot_aqua':\n",
    "            hotspots_aqua.append(data)\n",
    "        elif producer_id == 'producer_hotspot_terra':\n",
    "            hotspots_terra.append(data)\n",
    "\n",
    "    # Analyse hotspots data, find if any are close by & merge.\n",
    "    hotspots = hotspots_handler(hotspots_aqua, hotspots_terra)\n",
    "    # Merge hotspots with climate (depending if close & label if natural or other)\n",
    "    climate = climate_handler(climate, hotspots)\n",
    "\n",
    "    return climate\n",
    "\n",
    "\n",
    "def prepareForDB(data):\n",
    "    # Create new document dictionary (final version for DB) and clean up variables.\n",
    "    document = {}\n",
    "\n",
    "    document['date'] = datetime.datetime.fromisoformat(data['created_date'])\n",
    "    document['station'] = data['station']\n",
    "    document[\"air_temperature_celcius\"] = data['air_temperature_celcius']\n",
    "    document['relative_humidity'] = data['relative_humidity']\n",
    "    document['windspeed_knots'] = data['windspeed_knots']\n",
    "    document['max_wind_speed'] = data['max_wind_speed']\n",
    "    document['precipitation'] = data['precipitation']\n",
    "    document['precipitation_type'] = data['precipitation_type']\n",
    "    document['ghi'] = data['ghi']\n",
    "\n",
    "    if 'hotspots' in data:\n",
    "        document['hotspots'] = []\n",
    "        for each in data['hotspots']:\n",
    "            hotspot = {}\n",
    "            hotspot['time'] = datetime.datetime.fromisoformat(\n",
    "                each['created_time'])\n",
    "            hotspot['cause'] = each['cause']\n",
    "            hotspot['confidence'] = each['confidence']\n",
    "            hotspot['latitude'] = each['latitude']\n",
    "            hotspot['longitude'] = each['longitude']\n",
    "            hotspot['surface_temperature_celcius'] = each['surface_temperature_celcius']\n",
    "            document['hotspots'].append(hotspot)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "\n",
    "    data_batch = iter.collect()  # Returns all elements as an array.\n",
    "    # Send stream data to be transformed & analysed.\n",
    "    climate_data = stream_handler(data_batch)\n",
    "\n",
    "    # Sometimes batches may have no data, so we ensure that it isn't saved to database.\n",
    "    if len(climate_data) > 1:\n",
    "        # Send to remove key values that aren't in data-model, such as 'geo_hash'\n",
    "        database_data = prepareForDB(climate_data)\n",
    "\n",
    "        client = MongoClient()\n",
    "        db = client.fit3182_assignment_db\n",
    "        collection = db.climate\n",
    "\n",
    "        # Insert climate data into database.\n",
    "        collection.insert_one(database_data)\n",
    "        pprint.pprint(database_data)\n",
    "\n",
    "        client.close()\n",
    "\n",
    "\n",
    "batch_interval = 10\n",
    "topic = [\"Climate\", \"Hotspot_AQUA\", \"Hotspot_TERRA\"]\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, topic, {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'climate_report',\n",
    "    'fetch.message.max.bytes': '15728640',\n",
    "    'auto.offset.reset': 'largest'})\n",
    "\n",
    "# In batches every 10 seconds, set data to sendDatatoDB function.\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: sendDataToDB(rdd))\n",
    "\n",
    "ssc.start()\n",
    "# Run stream for 10 minutes just in case no detection of producer\n",
    "time.sleep(600)\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}